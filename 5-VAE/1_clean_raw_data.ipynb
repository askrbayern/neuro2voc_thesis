{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = os.path.join(\"m:\", \"neuro2voc\", \"raw_data\", \"j8v8\", \"20200228_02\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load frequencies\n",
    "raw_frequencies = np.load(os.path.join(data_dir, 'frequencies.spec.npy'))\n",
    "frequencies = raw_frequencies[0]\n",
    "\n",
    "# Load and pad spectrograms\n",
    "spec_files = ['raw_g0_t0.nidq.bin.spec.npy', 'raw_g1_t0.nidq.bin.spec.npy', 'raw_g2_t0.nidq.bin.spec.npy']\n",
    "padded_specs = []\n",
    "\n",
    "for spec_file in spec_files:\n",
    "    spec = np.load(os.path.join(data_dir, spec_file))\n",
    "    padding = 4  # 16ms * 250Hz = 40 samples\n",
    "    padded_spec = np.pad(spec, ((0, 0), (padding, 0)), mode='constant')\n",
    "    padded_specs.append(padded_spec)\n",
    "\n",
    "padded_spec_0, padded_spec_1, padded_spec_2 = padded_specs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Padded spec_0 shape: (128, 799315)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Padded spec_0 shape: {padded_spec_0.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and process annotations\n",
    "annotation_files = ['annotations_raw_g0_t0.nidq.bin.csv', 'annotations_raw_g1_t0.nidq.bin.csv', 'annotations_raw_g2_t0.nidq.bin.csv']\n",
    "filtered_dfs = []\n",
    "\n",
    "for annotation_file in annotation_files:\n",
    "    annotations_df = pd.read_csv(os.path.join(data_dir, annotation_file))\n",
    "    annotations_df['onset_sec'] = annotations_df['onset'] / 20000\n",
    "    annotations_df['duration_sec'] = annotations_df['duration'] / 20000\n",
    "    filtered_df = annotations_df[annotations_df['cluster_id'].between(2, 8)]\n",
    "    filtered_dfs.append(filtered_df)\n",
    "\n",
    "filtered_df_0, filtered_df_1, filtered_df_2 = filtered_dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>onset</th>\n",
       "      <th>duration</th>\n",
       "      <th>cluster_id</th>\n",
       "      <th>onset_sec</th>\n",
       "      <th>duration_sec</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>raw_g0_t0.nidq.bin</td>\n",
       "      <td>15521214.0</td>\n",
       "      <td>986.0</td>\n",
       "      <td>2</td>\n",
       "      <td>776.06070</td>\n",
       "      <td>0.04930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>raw_g0_t0.nidq.bin</td>\n",
       "      <td>15525652.0</td>\n",
       "      <td>979.0</td>\n",
       "      <td>2</td>\n",
       "      <td>776.28260</td>\n",
       "      <td>0.04895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>raw_g0_t0.nidq.bin</td>\n",
       "      <td>15576333.0</td>\n",
       "      <td>929.0</td>\n",
       "      <td>2</td>\n",
       "      <td>778.81665</td>\n",
       "      <td>0.04645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>raw_g0_t0.nidq.bin</td>\n",
       "      <td>15583214.0</td>\n",
       "      <td>991.0</td>\n",
       "      <td>2</td>\n",
       "      <td>779.16070</td>\n",
       "      <td>0.04955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>raw_g0_t0.nidq.bin</td>\n",
       "      <td>15588407.0</td>\n",
       "      <td>2960.0</td>\n",
       "      <td>3</td>\n",
       "      <td>779.42035</td>\n",
       "      <td>0.14800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  file       onset  duration  cluster_id  onset_sec  \\\n",
       "15  raw_g0_t0.nidq.bin  15521214.0     986.0           2  776.06070   \n",
       "16  raw_g0_t0.nidq.bin  15525652.0     979.0           2  776.28260   \n",
       "17  raw_g0_t0.nidq.bin  15576333.0     929.0           2  778.81665   \n",
       "18  raw_g0_t0.nidq.bin  15583214.0     991.0           2  779.16070   \n",
       "19  raw_g0_t0.nidq.bin  15588407.0    2960.0           3  779.42035   \n",
       "\n",
       "    duration_sec  \n",
       "15       0.04930  \n",
       "16       0.04895  \n",
       "17       0.04645  \n",
       "18       0.04955  \n",
       "19       0.14800  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_df_0.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Spike Times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load spike times\n",
    "spike_times_files = ['spiketimes_raw_g0_t0.nidq.bin.npy', 'spiketimes_raw_g1_t0.nidq.bin.npy', 'spiketimes_raw_g2_t0.nidq.bin.npy']\n",
    "spike_times = []\n",
    "\n",
    "for spike_file in spike_times_files:\n",
    "    spike_time = np.load(os.path.join(data_dir, spike_file))[0]\n",
    "    spike_times.append(spike_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File raw_g0_t0.nidq.bin.spec.npy duration: 53.29 minutes\n",
      "File spiketimes_raw_g0_t0.nidq.bin.npy duration: 53.29 minutes\n",
      "Number of annotations for g0: 417\n",
      "\n",
      "File raw_g1_t0.nidq.bin.spec.npy duration: 90.06 minutes\n",
      "File spiketimes_raw_g1_t0.nidq.bin.npy duration: 89.91 minutes\n",
      "Number of annotations for g1: 191\n",
      "\n",
      "File raw_g2_t0.nidq.bin.spec.npy duration: 77.90 minutes\n",
      "File spiketimes_raw_g2_t0.nidq.bin.npy duration: 70.37 minutes\n",
      "Number of annotations for g2: 2851\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print some information about the loaded data\n",
    "for i, (spec, filtered_df, spike_time) in enumerate(zip(padded_specs, filtered_dfs, spike_times)):\n",
    "    spec_duration = spec.shape[1] / (250*60)\n",
    "    spike_duration = spike_time.max() / (20000*60)\n",
    "    print(f\"File raw_g{i}_t0.nidq.bin.spec.npy duration: {spec_duration:.2f} minutes\")\n",
    "    print(f\"File spiketimes_raw_g{i}_t0.nidq.bin.npy duration: {spike_duration:.2f} minutes\")\n",
    "    print(f\"Number of annotations for g{i}: {len(filtered_df)}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "63896708.468520574"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spike_times[0][-10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process Spike Times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "spike_places_files = ['spikecluster_raw_g0_t0.nidq.bin.npy', 'spikecluster_raw_g1_t0.nidq.bin.npy', 'spikecluster_raw_g2_t0.nidq.bin.npy']\n",
    "\n",
    "spike_dfs = []\n",
    "\n",
    "for times_file, places_file in zip(spike_times_files, spike_places_files):\n",
    "    times = np.load(os.path.join(data_dir, times_file))[0]\n",
    "    places = np.load(os.path.join(data_dir, places_file))[0]\n",
    "    \n",
    "    # Convert times to integers and adjust sampling rate from 20000 Hz to 30000 Hz\n",
    "    # times = np.round(times * 30000 / 20000).astype(int)\n",
    "    # neural_resolution = \"30k\"\n",
    "    \n",
    "    times = np.round(times)\n",
    "    neural_resolution = \"20k\"\n",
    "    # Create a DataFrame for this file\n",
    "    df = pd.DataFrame({'time': times, 'place': places})\n",
    "    spike_dfs.append(df)\n",
    "\n",
    "# Unpack the DataFrames\n",
    "df_0, df_1, df_2 = spike_dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>place</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1463.0</td>\n",
       "      <td>326.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15893.0</td>\n",
       "      <td>326.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20591.0</td>\n",
       "      <td>326.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>25481.0</td>\n",
       "      <td>326.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>29877.0</td>\n",
       "      <td>326.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      time  place\n",
       "0   1463.0  326.0\n",
       "1  15893.0  326.0\n",
       "2  20591.0  326.0\n",
       "3  25481.0  326.0\n",
       "4  29877.0  326.0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_0.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data 0:\n",
      "Duplicated spike times with the same place found:\n",
      "               time  place\n",
      "1709741   2412097.0     31\n",
      "1709742   2412097.0     31\n",
      "1200553   5969994.0     27\n",
      "1200554   5969994.0     27\n",
      "1991691   5972994.0     40\n",
      "1991692   5972994.0     40\n",
      "73586     9623895.0    315\n",
      "73587     9623895.0    315\n",
      "2433316  11996845.0     55\n",
      "2433317  11996845.0     55\n",
      "632104   12627972.0     21\n",
      "632105   12627972.0     21\n",
      "79349    18724584.0    315\n",
      "79350    18724584.0    315\n",
      "237957   22453711.0    307\n",
      "237958   22453711.0    307\n",
      "1840560  38588677.0     35\n",
      "1840561  38588677.0     35\n",
      "291478   40034080.0    377\n",
      "291479   40034080.0    377\n",
      "3298932  46195650.0    327\n",
      "3298933  46195650.0    327\n",
      "Total number of duplicates: 22\n",
      "DataFrame shape before removing duplicates: (3318179, 2)\n",
      "Removed 11 duplicate rows.\n",
      "DataFrame shape after removing duplicates: (3318168, 2)\n",
      "\n",
      "Data 1:\n",
      "Duplicated spike times with the same place found:\n",
      "                time  place\n",
      "3083952   39213386.0     35\n",
      "3083953   39213386.0     35\n",
      "15440     40922947.0    326\n",
      "15441     40922947.0    326\n",
      "2937610   55860071.0     31\n",
      "2937611   55860071.0     31\n",
      "3310111   68632622.0     40\n",
      "3310112   68632622.0     40\n",
      "770652    83493553.0     19\n",
      "770653    83493553.0     19\n",
      "3427568   85398392.0    416\n",
      "3427569   85398392.0    416\n",
      "3524799   85575537.0     46\n",
      "3524800   85575537.0     46\n",
      "3032809   86304699.0    406\n",
      "3032810   86304699.0    406\n",
      "2646360   88734125.0     27\n",
      "2646361   88734125.0     27\n",
      "36309     99802163.0    326\n",
      "36310     99802163.0    326\n",
      "1986544  105396563.0     25\n",
      "1986545  105396563.0     25\n",
      "573651   107008759.0    307\n",
      "573652   107008759.0    307\n",
      "Total number of duplicates: 24\n",
      "DataFrame shape before removing duplicates: (4899009, 2)\n",
      "Removed 12 duplicate rows.\n",
      "DataFrame shape after removing duplicates: (4898997, 2)\n",
      "\n",
      "Data 2:\n",
      "Duplicated spike times with the same place found:\n",
      "               time  place\n",
      "2859399   8718144.0    403\n",
      "2859400   8718144.0    403\n",
      "1122512  11560497.0     21\n",
      "1122513  11560497.0     21\n",
      "1764299  20763865.0     25\n",
      "1764300  20763865.0     25\n",
      "1205851  21830627.0     21\n",
      "1205852  21830627.0     21\n",
      "279322   29305442.0     12\n",
      "279323   29305442.0     12\n",
      "43051    31109029.0    311\n",
      "43052    31109029.0    311\n",
      "2351030  34416876.0     27\n",
      "2351031  34416876.0     27\n",
      "3542354  38953092.0     48\n",
      "3542355  38953092.0     48\n",
      "3475768  41835597.0     47\n",
      "3475769  41835597.0     47\n",
      "693234   45394311.0    385\n",
      "693235   45394311.0    385\n",
      "4168887  46838849.0     63\n",
      "4168888  46838849.0     63\n",
      "3671218  49527139.0    293\n",
      "3671219  49527139.0    293\n",
      "820632   50522876.0     19\n",
      "820633   50522876.0     19\n",
      "1489644  57908342.0     21\n",
      "1489645  57908342.0     21\n",
      "4835521  60280511.0    425\n",
      "4835522  60280511.0    425\n",
      "3021528  61116355.0     36\n",
      "3021529  61116355.0     36\n",
      "4865778  67337515.0    319\n",
      "4865779  67337515.0    319\n",
      "3509481  69095603.0     47\n",
      "3509482  69095603.0     47\n",
      "1016057  69442014.0     20\n",
      "1016058  69442014.0     20\n",
      "504587   74588181.0     12\n",
      "504588   74588181.0     12\n",
      "3329323  83077723.0     44\n",
      "3329324  83077723.0     44\n",
      "Total number of duplicates: 42\n",
      "DataFrame shape before removing duplicates: (5020652, 2)\n",
      "Removed 21 duplicate rows.\n",
      "DataFrame shape after removing duplicates: (5020631, 2)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Find and remove duplicates with the same time and place\n",
    "for i in range(3):\n",
    "    df = spike_dfs[i]\n",
    "    df['place'] = df['place'].astype(int)\n",
    "    duplicates = df[df.duplicated(subset=['time', 'place'], keep=False)].sort_values(['time', 'place'])\n",
    "    print(f\"Data {i}:\")\n",
    "    \n",
    "    if len(duplicates) > 0:\n",
    "        print(\"Duplicated spike times with the same place found:\")\n",
    "        print(duplicates)\n",
    "        print(f\"Total number of duplicates: {len(duplicates)}\")\n",
    "        \n",
    "        # Remove duplicates\n",
    "        print(f\"DataFrame shape before removing duplicates: {df.shape}\")\n",
    "        df_deduped = df.drop_duplicates(subset=['time', 'place'], keep='first')\n",
    "        removed_count = len(df) - len(df_deduped)\n",
    "        spike_dfs[i] = df_deduped\n",
    "        print(f\"Removed {removed_count} duplicate rows.\")\n",
    "        print(f\"DataFrame shape after removing duplicates: {df_deduped.shape}\")\n",
    "    else:\n",
    "        print(\"No duplicated spike times with the same place found.\")\n",
    "    print()\n",
    "\n",
    "# Update individual DataFrames\n",
    "df_0, df_1, df_2 = spike_dfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now convert the spike places into index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data 0: Number of unique places: 75\n",
      "Unique place values: [12, 19, 20, 21, 25, 27, 31, 34, 35, 36, 38, 39, 40, 41, 44, 45, 46, 47, 48, 49, 50, 51, 52, 54, 55, 56, 57, 58, 59, 60, 61, 63, 64, 65, 66, 67, 68, 69, 72, 73, 81, 85, 92, 106, 107, 109, 110, 111, 284, 287, 291, 293, 298, 307, 311, 315, 319, 326, 327, 330, 346, 356, 370, 377, 385, 389, 396, 401, 403, 406, 411, 416, 419, 425, 428]\n",
      "\n",
      "Data 1: Number of unique places: 75\n",
      "Unique place values: [12, 19, 20, 21, 25, 27, 31, 34, 35, 36, 38, 39, 40, 41, 44, 45, 46, 47, 48, 49, 50, 51, 52, 54, 55, 56, 57, 58, 59, 60, 61, 63, 64, 65, 66, 67, 68, 69, 72, 73, 81, 85, 92, 106, 107, 109, 110, 111, 284, 287, 291, 293, 298, 307, 311, 315, 319, 326, 327, 330, 346, 356, 370, 377, 385, 389, 396, 401, 403, 406, 411, 416, 419, 425, 428]\n",
      "\n",
      "Data 2: Number of unique places: 75\n",
      "Unique place values: [12, 19, 20, 21, 25, 27, 31, 34, 35, 36, 38, 39, 40, 41, 44, 45, 46, 47, 48, 49, 50, 51, 52, 54, 55, 56, 57, 58, 59, 60, 61, 63, 64, 65, 66, 67, 68, 69, 72, 73, 81, 85, 92, 106, 107, 109, 110, 111, 284, 287, 291, 293, 298, 307, 311, 315, 319, 326, 327, 330, 346, 356, 370, 377, 385, 389, 396, 401, 403, 406, 411, 416, 419, 425, 428]\n",
      "\n",
      "Number of unique places across all DataFrames: 75\n"
     ]
    }
   ],
   "source": [
    "for i, df in enumerate(spike_dfs):\n",
    "    unique_places = df['place'].nunique()\n",
    "    print(f\"Data {i}: Number of unique places: {unique_places}\")\n",
    "    print(f\"Unique place values: {sorted(df['place'].unique())}\")\n",
    "    print()\n",
    "    \n",
    "unique_places = set()\n",
    "for df in spike_dfs:\n",
    "    unique_places.update(df['place'].unique())\n",
    "unique_places = sorted(unique_places)\n",
    "\n",
    "print(f\"Number of unique places across all DataFrames: {len(unique_places)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_info_path = os.path.join(data_dir, \"cluster_info.tsv\")\n",
    "cluster_info = pd.read_csv(cluster_info_path, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of rows in sorted_cluster_info: 75\n",
      "\n",
      "Reindexed Sorted Cluster Info:\n",
      "     id  Amplitude  ContamPct KSLabel       amp   ch  depth         fr group  \\\n",
      "0   326       43.8       52.3     mua  3.532561   18    200   7.026394   mua   \n",
      "1   315       47.8       43.4     mua  3.426856   47    480  10.556721   mua   \n",
      "2   311       47.8       43.4     mua  3.847038   47    480  12.385760   mua   \n",
      "3    12       71.5        2.9    good  6.398156   65    660  72.496604  good   \n",
      "4   307       41.3       81.2     mua  3.364714   82    840   8.601645   mua   \n",
      "..  ...        ...        ...     ...       ...  ...    ...        ...   ...   \n",
      "70  346       46.8       41.3     mua  3.699689  297   2980   4.820183   mua   \n",
      "71  330       46.6       99.4     mua  3.491918  317   3180   3.833593   mua   \n",
      "72   92       47.3       12.4     mua  3.606650  318   3200   5.159582   mua   \n",
      "73  327       45.5       52.4     mua  2.837812  321   3220   1.932522   mua   \n",
      "74  428       57.1       20.8     mua  4.542483  337   3380   5.999456   mua   \n",
      "\n",
      "    n_spikes  sh  new_index  \n",
      "0      90035   0          0  \n",
      "1     135272   0          1  \n",
      "2     158709   0          2  \n",
      "3     928959   0          3  \n",
      "4     110220   0          4  \n",
      "..       ...  ..        ...  \n",
      "70     61765   0         70  \n",
      "71     49123   0         71  \n",
      "72     66114   0         72  \n",
      "73     24763   0         73  \n",
      "74     76876   0         74  \n",
      "\n",
      "[75 rows x 12 columns]\n",
      "\n",
      "Number of rows (returned): 75\n"
     ]
    }
   ],
   "source": [
    "filtered_cluster_info = cluster_info[cluster_info['id'].isin(unique_places)]\n",
    "sorted_cluster_info = filtered_cluster_info.sort_values('depth')\n",
    "\n",
    "num_rows = len(sorted_cluster_info)\n",
    "print(f\"\\nNumber of rows in sorted_cluster_info: {num_rows}\")\n",
    "\n",
    "# Reindex with a new index\n",
    "sorted_cluster_info.reset_index(drop=True, inplace=True)\n",
    "sorted_cluster_info['new_index'] = sorted_cluster_info.index\n",
    "\n",
    "# Print reindexed sorted_cluster_info\n",
    "print(\"\\nReindexed Sorted Cluster Info:\")\n",
    "print(sorted_cluster_info)\n",
    "\n",
    "print(f\"\\nNumber of rows (returned): {num_rows}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapping sample:\n",
      "[(326, 0), (315, 1), (311, 2), (12, 3), (307, 4), (377, 5), (356, 6), (385, 7), (370, 8), (20, 9)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "id_to_new_index = dict(zip(sorted_cluster_info['id'], sorted_cluster_info['new_index']))\n",
    "print(\"Mapping sample:\")\n",
    "print(list(id_to_new_index.items())[:10])\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data 0: Added reindexed_place\n",
      "Number of unique reindexed_place values: 75\n",
      "reindexed_place range: 0 to 74\n",
      "Number of NaN values in reindexed_place: 0\n",
      "\n",
      "Data 1: Added reindexed_place\n",
      "Number of unique reindexed_place values: 75\n",
      "reindexed_place range: 0 to 74\n",
      "Number of NaN values in reindexed_place: 0\n",
      "\n",
      "Data 2: Added reindexed_place\n",
      "Number of unique reindexed_place values: 75\n",
      "reindexed_place range: 0 to 74\n",
      "Number of NaN values in reindexed_place: 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def reindex_place(df):\n",
    "    df['reindexed_place'] = df['place'].map(id_to_new_index)\n",
    "    return df\n",
    "\n",
    "for i, df in enumerate(spike_dfs):\n",
    "    spike_dfs[i] = reindex_place(df)\n",
    "    print(f\"Data {i}: Added reindexed_place\")\n",
    "    print(f\"Number of unique reindexed_place values: {df['reindexed_place'].nunique()}\")\n",
    "    print(f\"reindexed_place range: {df['reindexed_place'].min()} to {df['reindexed_place'].max()}\")\n",
    "    print(f\"Number of NaN values in reindexed_place: {df['reindexed_place'].isna().sum()}\")\n",
    "    print()\n",
    "\n",
    "# Update individual DataFrames\n",
    "df_0, df_1, df_2 = spike_dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>place</th>\n",
       "      <th>reindexed_place</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3318174</th>\n",
       "      <td>63917341.0</td>\n",
       "      <td>428</td>\n",
       "      <td>74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3318175</th>\n",
       "      <td>63918320.0</td>\n",
       "      <td>428</td>\n",
       "      <td>74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3318176</th>\n",
       "      <td>63919842.0</td>\n",
       "      <td>428</td>\n",
       "      <td>74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3318177</th>\n",
       "      <td>63933035.0</td>\n",
       "      <td>428</td>\n",
       "      <td>74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3318178</th>\n",
       "      <td>63942756.0</td>\n",
       "      <td>428</td>\n",
       "      <td>74</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               time  place  reindexed_place\n",
       "3318174  63917341.0    428               74\n",
       "3318175  63918320.0    428               74\n",
       "3318176  63919842.0    428               74\n",
       "3318177  63933035.0    428               74\n",
       "3318178  63942756.0    428               74"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_0.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = os.path.join(\"m:\", \"neuro2voc\", \"task_5\", \"data\")\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# Save each DataFrame to a CSV file\n",
    "# for i, df in enumerate([df_0, df_1, df_2]):\n",
    "#     file_name = f\"processed_spikes_{i}_{neural_resolution}.csv\"\n",
    "#     file_path = os.path.join(save_dir, file_name)\n",
    "#     df.to_csv(file_path, index=False)\n",
    "#     print(f\"Saved DataFrame {i} to {file_path}\")\n",
    "\n",
    "# print(\"All DataFrames have been saved successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dataframes(dataframes, output_dir, merge=True):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    if merge:\n",
    "        spike_times = np.concatenate([df['time'].values for df in dataframes])\n",
    "        neuron_ids = np.concatenate([df['reindexed_place'].values for df in dataframes])\n",
    "        \n",
    "        max_time = int(np.ceil(spike_times.max()))\n",
    "        num_neurons = neuron_ids.max() + 1\n",
    "        time_series = np.zeros((num_neurons, max_time), dtype=np.int16)\n",
    "        \n",
    "        for time, neuron in zip(spike_times, neuron_ids):\n",
    "            time_index = int(time)\n",
    "            if time_index < max_time:\n",
    "                time_series[neuron, time_index] = 1\n",
    "        \n",
    "        time_series.tofile(os.path.join(output_dir, 'neural_data_merged.bin'))\n",
    "        print(f\"Merged data saved. Shape: {time_series.shape}\")\n",
    "    \n",
    "    else:\n",
    "        for i, df in enumerate(dataframes):\n",
    "            spike_times = df['time'].values\n",
    "            neuron_ids = df['reindexed_place'].values\n",
    "            \n",
    "            max_time = int(np.ceil(spike_times.max()))\n",
    "            num_neurons = neuron_ids.max() + 1\n",
    "            time_series = np.zeros((num_neurons, max_time), dtype=np.int16)\n",
    "            \n",
    "            for time, neuron in zip(spike_times, neuron_ids):\n",
    "                time_index = int(time)\n",
    "                if time_index < max_time:\n",
    "                    time_series[neuron, time_index] = 1\n",
    "            \n",
    "            time_series.tofile(os.path.join(output_dir, f'neural_data_2_{neural_resolution}.bin'))\n",
    "            print(f\"Data 2 has been saved. Shape: {time_series.shape}\")\n",
    "    \n",
    "    channel_map = np.arange(num_neurons, dtype=np.int32)\n",
    "    np.save(os.path.join(output_dir, 'channel_map.npy'), channel_map)\n",
    "    print(f\"Channel mapping has been saved. Number of chanels: {num_neurons}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data 2 has been saved. Shape: (75, 84439846)\n",
      "Channel mapping has been saved. Number of chanels: 75\n"
     ]
    }
   ],
   "source": [
    "process_dataframes([df_2], save_dir, merge=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
